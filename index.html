<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>Homepage of Fuyan Ma</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <!--[if lte IE 8]>
    <script src="assets/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
          integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/css/main.css"/>
    <!--[if lte IE 8]>
    <link rel="stylesheet" href="assets/css/ie8.css"/><![endif]-->
</head>

<body>

<!-- Header -->
<section id="header">
    <header>
        <span class="image avatar"><img src="images/fuyanma.png" alt=""/></span>
        <h1 id="logo"><a href="#">Fuyan Ma (马付严)</a></h1>
        <p>Changsha, China</p>
    </header>
    <nav id="nav">
        <ul>
            <li><a href="#two">News</a></li>
            <li><a href="#three">Publications</a></li>
            <li><a href="#four">Awards</a></li>
        </ul>
    </nav>
    <footer>
        <ul class="icons">
            <li><a href="https://github.com/MaFuyan/" class="icon fa-github"><span class="label">Github</span></a></li>
            <li><a href="mailto:mafuyan@hnu.edu.cn" class="icon fa-envelope"><span class="label">Email</span></a></li>
        </ul>
    </footer>
</section>

<!-- Wrapper -->
<div id="wrapper">

    <!-- Main -->
    <div id="main">

        <!-- One -->
        <section id="one">
            <div class="container">
                <header class="major">
                    <h3>Fuyan Ma</h3>
                    <p>PhD Candidate, Hunan University
                        <!--                        <br>Changsha, China</br>-->
                    </p>

                </header>
                <p>I'm a fourth-year PhD student in the College of Electrical and Information Engineering of <a
                        href="https://www.hnu.edu.cn/">Hunan University</a> (HNU), supervised by Prof. <a
                        href="http://eeit.hnu.edu.cn/info/1279/5237.htm">Shutao Li</a> and Prof. <a
                        href="http://eeit.hnu.edu.cn/info/1549/8165.htm">Bin Sun</a>. Before pursing the PhD degree in
                    HNU, I obtained my bachelor's degree from <a href="https://www.scuec.edu.cn/">South Central
                        University for Nationalities</a> in 2018. My research interests include affective computing,
                    computer vision and multimodal fusion.</p>
            </div>
        </section>

        <!-- Two -->
        <section id="two">
            <div class="container">
                <h3>News</h3>
                <ul>
                    <li>
                        10/2021 : One Journal paper has been accepted by IEEE-TAFFC.
                    </li>
                    <li>
                        09/2021 : One Conference paper has been accepted by ACM-MM2021 Workshop.
                    </li>
                    <li>
                        04/2020 : One Conference paper has been accepted by IGARSS2020.
                    </li>

                </ul>
            </div>
        </section>

        <!-- Three -->
        <section id="three">
            <div class="container">
                <h3>Publications</h3>
                <ol>


                    <li>
                        <div id="ma2021robust"><em><strong>Fuyan Ma</strong>, Bin Sun and Shutao Li</em>, <strong>Facial
                            Expression
                            Recognition with Visual Transformers and Attentional Selective Fusion</strong>, IEEE
                            Transactions on Affective Computing (2021).
                        </div>
                        <div>[<a data-toggle="collapse" href="#abs_ma2021robust">Abstract</a>][<a
                                href="#bib_ma2021robust" data-toggle="collapse">BibTeX</a>] [<a
                                href="https://doi.org/10.1109/TAFFC.2021.3122146" target="_blank">DOI</a>] [<a
                                href="to be done" target="_blank">URL</a>] [<a href="documents/TAFFC-2021-VTFF.pdf"
                                                                               target="_blank">PDF</a>]
                        </div>

                        <div id="abs_ma2021robust" class="collapse">
                            <b>Abstract</b>:
                            Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions,
                            variant head poses, face deformation and motion blur under unconstrained conditions.
                            Although substantial progresses have been made in automatic FER in the past few decades,
                            previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant
                            head poses and other issues definitely increase the difficulty of FER on account of these
                            information-deficient regions and complex backgrounds. Different from previous pure CNNs
                            based methods, we argue that it is feasible and practical to translate facial images into
                            sequences of visual words and perform expression recognition from a global perspective.
                            Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in
                            the wild by two main steps. First, we propose the attentional selective fusion (ASF) for
                            leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures
                            discriminative information by fusing multiple features with the global-local attention. The
                            fused feature maps are then flattened and projected into sequences of visual words. Second,
                            inspired by the success of Transformers in natural language processing, we propose to model
                            relationships between these visual words with the global self-attention. The proposed method
                            is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and
                            AffectNet). Under the same settings, extensive experiments demonstrate that our method shows
                            superior performance over other methods, setting new state of the art on RAF-DB with 88.14%,
                            FERPlus with 88.81% and AffectNet with 61.85%. The cross-dataset evaluation on CK+ shows the
                            promising generalization capability of the proposed method.
                        </div>
                        <div id="bib_ma2021robust" class="collapse">
                            <b>BibTeX</b>:
                            <pre>
@article{ma2021robust,
  title={Robust facial expression recognition with convolutional visual transformers},
  author={Ma, Fuyan and Sun, Bin and Li, Shutao},
  journal={arXiv preprint arXiv:2103.16854},
  year={2021}
}
</pre>
                        </div>
                    </li>

                    <!--------------------------------------------------------------------------------------------------------------------->
                    <li>
                        <div id="ma2021hybrid"><em>Ziyu Ma<sub>*</sub>, <strong>Fuyan Ma<sub>*</sub></strong>, Bin Sun
                            and Shutao Li</em>, <strong>Hybrid
                            Mutimodal Fusion for Dimensional Emotion Recognition</strong>, Proceedings of the 2nd on
                            Multimodal Sentiment Analysis Challenge (2021).
                        </div>
                        <div>[<a data-toggle="collapse" href="#abs_ma2021hybrid">Abstract</a>][<a
                                href="#bib_ma2021hybrid" data-toggle="collapse">BibTeX</a>] [<a
                                href="https://doi.org/10.1145/3475957.3484457" target="_blank">DOI</a>] [<a
                                href="https://dl.acm.org/doi/10.1145/3475957.3484457" target="_blank">URL</a>] [<a
                                href="documents/MUSE-2021.pdf"
                                target="_blank">PDF</a>]
                        </div>

                        <div id="abs_ma2021hybrid" class="collapse">
                            <b>Abstract</b>:
                            In this paper, we extensively present our solutions for the MuSe-Stress sub-challenge
                            and the MuSe-Physio sub-challenge of Multimodal Sentiment Challenge (MuSe) 2021. The
                            goal of MuSe-Stress sub-challenge is to predict the level of emotional arousal and
                            valence in a time-continuous manner from audio-visual recordings and the goal of MuSe-Physio
                            sub-challenge is to predict the level of psycho-physiological arousal from a) human
                            annotations fused with b) galvanic skin response (also known as Electrodermal Activity
                            (EDA)) signals from the stressed people. The Ulm-TSST dataset which is a novel subset
                            of the audio-visual textual Ulm-Trier Social Stress dataset that features German speakers
                            in a Trier Social Stress Test (TSST) induced stress situation is used in both
                            sub-challenges.
                            For the MuSe-Stress sub-challenge, we highlight our solutions in three aspects: 1)
                            the audio-visual features and the bio-signal features are used for emotional state
                            recognition. 2) the Long Short-Term Memory (LSTM) with the self-attention mechanism
                            is utilized to capture complex temporal dependencies within the feature sequences.
                            3) the late fusion strategy is adopted to further boost the model's recognition performance
                            by exploiting complementary information scattered across multimodal sequences. Our
                            proposed model achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively
                            on the test set, which both rank in the top 3. For the MuSe-Physio sub-challenge,
                            we first extract the audio-visual features and the bio-signal features from multiple
                            modalities. Then, the LSTM module with the self-attention mechanism, and the Gated
                            Convolutional Neural Networks (GCNN) as well as the LSTM network are utilized for
                            modeling the complex temporal dependencies in the sequence. Finally, the late fusion
                            strategy is used. Our proposed method also achieves CCC of 0.5412 on the test set,
                            which ranks in the top 3.
                        </div>
                        <div id="bib_ma2021hybrid" class="collapse">
                            <b>BibTeX</b>:
                            <pre>
@incollection{ma2021hybrid,
  title={Hybrid Mutimodal Fusion for Dimensional Emotion Recognition},
  author={Ma, Ziyu and Ma, Fuyan and Sun, Bin and Li, Shutao},
  booktitle={Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge},
  pages={29--36},
  year={2021}
}
</pre>
                        </div>
                    </li>

                    <!--------------------------------------------------------------------------------------------------------------------->
                    <li>
                        <div id="ma2020vehicle"><em><strong>Fuyan Ma</strong>, Bin Sun, Shutao Li and Jun Sun</em>,
                            <strong>Vehicle Detection with Partial Anchors in Remote Sensing Images</strong>, IEEE
                            International Geoscience and Remote Sensing Symposium (2020).
                        </div>
                        <div>[<a data-toggle="collapse" href="#abs_ma2020vehicle">Abstract</a>][<a
                                href="#bib_ma2020vehicle" data-toggle="collapse">BibTeX</a>] [<a
                                href="https://doi.org/10.1109/IGARSS39084.2020.9323956" target="_blank">DOI</a>] [<a
                                href="https://ieeexplore.ieee.org/abstract/document/9323956" target="_blank">URL</a>]
                            [<a
                                    href="documents/IGARSS-2020.pdf"
                                    target="_blank">PDF</a>]
                        </div>

                        <div id="abs_ma2020vehicle" class="collapse">
                            <b>Abstract</b>:
                            Vehicle detection in remote sensing(RS) images has been an active topic with the development
                            of computer vision in recent years. However, directly applying conventional horizontal
                            anchor-based detection methods in oriented vehicle detection often acquires poor
                            performance. Although rotated anchors have been used to tackle this problem, this design
                            leads to heavy computational cost because of thousands of rotated anchors generated in each
                            level feature map. In this paper, we propose to detect vehicles with partial anchors, which
                            greatly accelerates detection process. The novel Partial Anchors based Detection
                            Network(PADeN) filter out redundant anchors with semantic information. To boost the
                            performance of PADeN, the centerness mask branch is added into the network. The results
                            demonstrate that PADeN significantly outperforms previous approaches in vehicle detection
                            and achieves the mAP of 76.9%.
                        </div>
                        <div id="bib_ma2020vehicle" class="collapse">
                            <b>BibTeX</b>:
                            <pre>
@inproceedings{ma2020vehicle,
  title={Vehicle Detection with Partial Anchors in Remote Sensing Images},
  author={Ma, Fuyan and Sun, Bin and Li, Shutao and Sun, Jun},
  booktitle={IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium},
  pages={288--291},
  year={2020},
  organization={IEEE}
}
</pre>
                        </div>
                    </li>

                </ol>

            </div>
        </section>

        <!-- Four -->
        <section id="four">
            <div class="container">
                <h3>Awards</h3>
                <ul>
                    <li>
                        2017.11 : Undergraduate National Scholarship.
                    </li>
                    <li>
                        2016.11 : Undergraduate National Scholarship.
                    </li>
                    <li>
                        2015.11 : Undergraduate National Scholarship.
                    </li>
                </ul>


            </div>
        </section>


    </div>


    <!-- Footer -->
    <section id="footer">
        <div class="container">
            <script type="text/javascript" id="clustrmaps"
                    src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=gdIoC1pOpD6eu1UoO2lKQN6cibQEUlFh5b-3kNPLL2w&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>
            <ul class="copyright">
                <li>&copy;All rights reserved.</li>
                <li>Last updated: 2021.10.21</a></li>
            </ul>
        </div>
    </section>

</div>


<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrollzer.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/skel.min.js"></script>
<script src="assets/js/util.js"></script>
<!--[if lte IE 8]>
<script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="assets/js/main.js"></script>
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
        integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
        crossorigin="anonymous"></script>
</body>
</html>
